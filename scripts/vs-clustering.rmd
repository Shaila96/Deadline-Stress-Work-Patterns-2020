---
title: "Clustering"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(grid)
library(gridExtra)
library(dplyr)
library(ggpubr)
#install.packages('DataCombine')
library(DataCombine)
library(dplyr)
library(reshape2)
library(tidyr)
library(stringr)
library(plyr)
library(ggplot2)
library(dplyr)
library(miceadds)
library(gtools)
library(ggpubr)
library(gvlma)
library(MASS)
library(reshape2)
library(ggcorrplot)
library(GGally)
library(ggnewscale)
library(scales)
library(cowplot)

library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(factoextra) # clustering algorithms & visualization

current_dir <- getwd()
setwd('..')
root_dir <- getwd()
data_dir <- file.path(root_dir, 'curated-data/physiological-data')
#data_file_name<-'qc99_all_subj.csv'
data_file_name<-'qc1_treatment_mean_v1.csv'
```


```{r,echo=FALSE}
DF_Bib<-read.csv("C:/Users/soton/Documents/R/DeadlineProject/Questionnaire/@CustomBiographic/Questionnaire Data.csv")
Main_DF<-read.csv(file.path(data_dir,data_file_name),stringsAsFactors = FALSE)
Main_DF<-subset(Main_DF, select = -c(E4_HR, E4_EDA,iWatch_HR ))

Cluster_DF <- data.frame(matrix(ncol = 5, nrow = 9))
x <- c("Participant_ID", "Day1", "Day2", "Day3", "Day4")
colnames(Cluster_DF) <- x

Cluster_DF$Participant_ID<-levels(factor(Main_DF$Participant_ID))
Day_No<-factor(Main_DF$Day)
numberOfDay=nlevels(Day_No)
      for (j in 1:numberOfDay) {
        
        Cluster_DF_RB<-subset(Main_DF, Day==levels(Day_No)[j] & Main_DF$Treatment=="RB")
        Cluster_DF_WS<-subset(Main_DF, Day==levels(Day_No)[j] & Main_DF$Treatment=="WS")
        Cluster_DF[j+1]<- Cluster_DF_WS$PP - Cluster_DF_RB$PP
        
      }



Cluster_DF$Trait_anxiety<- DF_Bib$Trait_anxiety
Cluster_DF$Personality<- DF_Bib$Personality_AB
rownames(Cluster_DF)<-Cluster_DF$Participant_ID
Cluster_DF$Participant_ID<-NULL

# Cluster_DF$Participant_ID<-levels(factor(Main_DF$Participant_ID))
# 
# Cluster_DF<-subset(Main_DF, Day=="Day1" & Main_DF$Treatment=="RB")
```

##  Clustering with PP and Trait Anxiety

```{r, echo=FALSE}
k2 <- kmeans(Cluster_DF, centers = 2, nstart = 25)
k3 <- kmeans(Cluster_DF, centers = 3, nstart = 25)
k4 <- kmeans(Cluster_DF, centers = 4, nstart = 25)
#k5 <- kmeans(Cluster_DF, centers = 5, nstart = 25)

# plots to compare
p1 <- fviz_cluster(k2, geom = "point", data = Cluster_DF) + ggtitle("k = 2")+ xlab("") + ylab("")+theme(plot.title = element_text(hjust = 0.5))
p2<- fviz_cluster(k2, data = Cluster_DF) + ggtitle("k = 2")+ xlab("") + ylab("")+theme(plot.title = element_text(hjust = 0.5))
p3 <- fviz_cluster(k3, geom = "point",  data = Cluster_DF) + ggtitle("k = 3")+ xlab("") + ylab("")+theme(plot.title = element_text(hjust = 0.5))
p4<- fviz_cluster(k3, data = Cluster_DF) +  ggtitle("k = 3")+ xlab("") + ylab("")+theme(plot.title = element_text(hjust = 0.5))
p5 <- fviz_cluster(k4, geom = "point",  data = Cluster_DF) + ggtitle("k = 4")+ xlab("") + ylab("")+theme(plot.title = element_text(hjust = 0.5))
p6<- fviz_cluster(k4, data = Cluster_DF) + ggtitle("k = 4")+ xlab("") + ylab("")+theme(plot.title = element_text(hjust = 0.5))
#p4 <- fviz_cluster(k5, geom = "point",  data = Cluster_DF) + ggtitle("k = 5")

library(gridExtra)
grid.arrange(p1, p2, p3,p4,p5,p6, nrow=3, ncol = 2)
```


## Determining Optimal Clusters
Elbow Method:
For each k, calculate the total within-cluster sum of square (wss)
Plot the curve of wss according to the number of clusters k.
The location of a bend (knee) in the plot is generally considered as an indicator of the appropriate number of clusters.

```{r, echo=FALSE}
set.seed(123)

# function to compute total within-cluster sum of square 
wss <- function(k) {
  kmeans(Cluster_DF, k, nstart = 25 )$tot.withinss
}

# Compute and plot wss for k = 1 to k = 15
k.values <- 1:4

# extract wss for 2-15 clusters
wss_values <- map_dbl(k.values, wss)

plot(k.values, wss_values,
       type="b", pch = 19, frame = FALSE, 
       main="Optimal Clusters using Elbow Method",
       xlab="Number of clusters K",
       ylab="Total within-clusters sum of squares")
```

Average Silhouette Method
In short, the average silhouette approach measures the quality of a clustering. That is, it determines how well each object lies within its cluster. A high average silhouette width indicates a good clustering. The average silhouette method computes the average silhouette of observations for different values of k. The optimal number of clusters k is the one that maximizes the average silhouette over a range of possible values for k^2

```{r, echo=FALSE}
avg_sil <- function(k) {
  km.res <- kmeans(Cluster_DF, centers = k, nstart = 25)
  ss <- silhouette(km.res$cluster, dist(Cluster_DF))
  mean(ss[, 3])
}

# Compute and plot wss for k = 2 to k = 15
k.values <- 2:5

# extract avg silhouette for 2-15 clusters
avg_sil_values <- map_dbl(k.values, avg_sil)

plot(k.values, avg_sil_values,
       type = "b", pch = 19, frame = FALSE, 
      main="Optimal Clusters using  Average silhouette Method",
       xlab = "Number of clusters K",
       ylab = "Average Silhouettes")
```



```{r, echo=FALSE}
set.seed(123)
final <- kmeans(Cluster_DF, 3, nstart = 25)
print(final)
fviz_cluster(final, data = Cluster_DF)+ggtitle("Cluster using PP-Trait; k = 2")+ xlab("") + ylab("")+theme(plot.title = element_text(hjust = 0.5))
```




```{r, echo=FALSE}
Cluster_DF$Trait_anxiety<-NULL
Cluster_DF$Agreeableness<- DF_Bib$BFI_.Agreeableness
Cluster_DF$Conscientiousness<- DF_Bib$BFI_.Conscientiousness
Cluster_DF$Extraversion<- DF_Bib$BFI_.Extraversion
Cluster_DF$Neuroticism<- DF_Bib$BFI_.Neuroticism
Cluster_DF$Openness<- DF_Bib$BFI_.Openness
```




##  Clustering with PP and BFI

```{r, echo=FALSE}
k2 <- kmeans(Cluster_DF, centers = 2, nstart = 25)
k3 <- kmeans(Cluster_DF, centers = 3, nstart = 25)
k4 <- kmeans(Cluster_DF, centers = 4, nstart = 25)
#k5 <- kmeans(Cluster_DF, centers = 5, nstart = 25)

# plots to compare
# plots to compare
p1 <- fviz_cluster(k2, geom = "point", data = Cluster_DF) + ggtitle("k = 2")+ xlab("") + ylab("")+theme(plot.title = element_text(hjust = 0.5))
p2<- fviz_cluster(k2, data = Cluster_DF) + ggtitle("k = 2")+ xlab("") + ylab("")+theme(plot.title = element_text(hjust = 0.5))
p3 <- fviz_cluster(k3, geom = "point",  data = Cluster_DF) + ggtitle("k = 3")+ xlab("") + ylab("")+theme(plot.title = element_text(hjust = 0.5))
p4<- fviz_cluster(k3, data = Cluster_DF) +  ggtitle("k = 3")+ xlab("") + ylab("")+theme(plot.title = element_text(hjust = 0.5))
p5 <- fviz_cluster(k4, geom = "point",  data = Cluster_DF) + ggtitle("k = 4")+ xlab("") + ylab("")+theme(plot.title = element_text(hjust = 0.5))
p6<- fviz_cluster(k4, data = Cluster_DF) + ggtitle("k = 4")+ xlab("") + ylab("")+theme(plot.title = element_text(hjust = 0.5))
#p4 <- fviz_cluster(k5, geom = "point",  data = Cluster_DF) + ggtitle("k = 5")

library(gridExtra)
grid.arrange(p1, p2, p3,p4,p5,p6, ncol = 2, nrow=3)
```


## Determining Optimal Clusters



```{r, echo=FALSE}
set.seed(123)

# function to compute total within-cluster sum of square 
wss <- function(k) {
  kmeans(Cluster_DF, k, nstart = 25 )$tot.withinss
}

# Compute and plot wss for k = 1 to k = 15
k.values <- 1:4

# extract wss for 2-15 clusters
wss_values <- map_dbl(k.values, wss)

plot(k.values, wss_values,
       type="b", pch = 19, frame = FALSE,
     main="Optimal Clusters using Elbow Method",
       xlab="Number of clusters K",
       ylab="Total within-clusters sum of squares")
```


```{r, echo=FALSE}
avg_sil <- function(k) {
  km.res <- kmeans(Cluster_DF, centers = k, nstart = 25)
  ss <- silhouette(km.res$cluster, dist(Cluster_DF))
  mean(ss[, 3])
}

# Compute and plot wss for k = 2 to k = 15
k.values <- 2:5

# extract avg silhouette for 2-15 clusters
avg_sil_values <- map_dbl(k.values, avg_sil)

plot(k.values, avg_sil_values,
       type = "b", pch = 19, frame = FALSE, 
           main="Optimal Clusters using  Average silhouette Method",
       xlab = "Number of clusters K",
       ylab = "Average Silhouettes")
```



```{r, echo=FALSE}
set.seed(123)
final <- kmeans(Cluster_DF, 2, nstart = 25)
print(final)
fviz_cluster(final, data = Cluster_DF)+ggtitle("Cluster using PP-BFI; k = 2")+ xlab("") + ylab("")+theme(plot.title = element_text(hjust = 0.5))
```



##  Clustering with PP, Trait Anxiety and BFI

```{r, echo=FALSE}
Cluster_DF$Trait_anxiety<- DF_Bib$Trait_anxiety
Cluster_DF$Personality<- DF_Bib$Personality_AB
k2 <- kmeans(Cluster_DF, centers = 2, nstart = 25)
k3 <- kmeans(Cluster_DF, centers = 3, nstart = 25)
k4 <- kmeans(Cluster_DF, centers = 4, nstart = 25)
#k5 <- kmeans(Cluster_DF, centers = 5, nstart = 25)

# plots to compare
p1 <- fviz_cluster(k2, geom = "point", data = Cluster_DF) + ggtitle("k = 2")+ xlab("") + ylab("")+theme(plot.title = element_text(hjust = 0.5))
p2<- fviz_cluster(k2, data = Cluster_DF) + ggtitle("k = 2")+ xlab("") + ylab("")+theme(plot.title = element_text(hjust = 0.5))
p3 <- fviz_cluster(k3, geom = "point",  data = Cluster_DF) + ggtitle("k = 3")+ xlab("") + ylab("")+theme(plot.title = element_text(hjust = 0.5))
p4<- fviz_cluster(k3, data = Cluster_DF) +  ggtitle("k = 3")+ xlab("") + ylab("")+theme(plot.title = element_text(hjust = 0.5))
p5 <- fviz_cluster(k4, geom = "point",  data = Cluster_DF) + ggtitle("k = 4")+ xlab("") + ylab("")+theme(plot.title = element_text(hjust = 0.5))
p6<- fviz_cluster(k4, data = Cluster_DF) + ggtitle("k = 4")+ xlab("") + ylab("")+theme(plot.title = element_text(hjust = 0.5))
#p4 <- fviz_cluster(k5, geom = "point",  data = Cluster_DF) + ggtitle("k = 5")

library(gridExtra)
grid.arrange(p1, p2, p3,p4,p5,p6, ncol = 2, nrow=3)
```


## Determining Optimal Clusters



```{r, echo=FALSE}
set.seed(123)

# function to compute total within-cluster sum of square 
wss <- function(k) {
  kmeans(Cluster_DF, k, nstart = 25 )$tot.withinss
}

# Compute and plot wss for k = 1 to k = 15
k.values <- 2:4

# extract wss for 2-15 clusters
wss_values <- map_dbl(k.values, wss)

plot(k.values, wss_values,
       type="b", pch = 19, frame = FALSE, 
          main="Optimal Clusters using Elbow Method",
       xlab="Number of clusters K",
       ylab="Total within-clusters sum of squares")
```


```{r, echo=FALSE}
avg_sil <- function(k) {
  km.res <- kmeans(Cluster_DF, centers = k, nstart = 25)
  ss <- silhouette(km.res$cluster, dist(Cluster_DF))
  mean(ss[, 3])
}

# Compute and plot wss for k = 2 to k = 15
k.values <- 2:4

# extract avg silhouette for 2-15 clusters
avg_sil_values <- map_dbl(k.values, avg_sil)

plot(k.values, avg_sil_values,
       type = "b", pch = 19, frame = FALSE, 
           main="Optimal Clusters using  Average silhouette Method",
       xlab = "Number of clusters K",
       ylab = "Average Silhouettes")
```



```{r, echo=FALSE}
set.seed(123)
final <- kmeans(Cluster_DF, 2, nstart = 25)
print(final)
fviz_cluster(final, data = Cluster_DF)+ggtitle("Cluster using PP-Trait-BFI; k = 2")+ xlab("") + ylab("")+theme(plot.title = element_text(hjust = 0.5))
```




```{r}
Activity_data <- data.frame(Participant_ID=character(),
                            Day=character(),
                            R=integer(),
                            W=integer(),
                            Out=integer(),
                            SA=integer(),
                            SP=integer(),
                            I=integer(), 
                 stringsAsFactors=FALSE) 
```

```{r}
current_dir <- getwd()
setwd('..')
root_dir <- getwd()
data_dir <- file.path(root_dir, 'curated-data/physiological-data')
data_file_name<-'qc99_all_subj.csv'

DF<-read.csv(file.path(data_dir,data_file_name),stringsAsFactors = FALSE)
file_name <- 'Quantitive_All_subject.csv'
DF<-DF[!(DF$Participant_ID=="T005" & DF$Day=="Day4"),]
DF$Day[DF$Day=="Day5"] <- "Day4"
```


```{r}
SubjectID<-factor(DF$Participant_ID)
numberSid=nlevels(SubjectID)
for (i in 1:numberSid) {
  #Timeline_Heatmap_list<-list()
  DF_Sub<-subset(DF, Participant_ID==levels(SubjectID)[i] & DF$Treatment=="WS")
  subject_title=levels(SubjectID)[i]

      
      Day_No<-factor(DF_Sub$Day)
      numberOfDay=nlevels(Day_No)
      
      for (j in 1:numberOfDay) {
        Title=levels(Day_No)[j]
        DF_Sub_Day<-subset(DF_Sub, Day==levels(Day_No)[j])
        DF_Sub_Day<-DF_Sub_Day[!is.na(DF_Sub_Day$Reduce_Activities_QC1),]
        DF_Sub_Day<-DF_Sub_Day[!is.na(DF_Sub_Day$Reduced_Ontology_One),]
        #Draw_Timeline_Heatmap(Day_DF,subject_title,Title)
        
        
        
          
    X<-c("R", "W", "Out","SA", "SP","I")
    print(X)
    tab_heatmap <- table(DF_Sub_Day$Reduce_Activities_QC1)
    DF_Sub_Day<-DF_Sub_Day[DF_Sub_Day$Reduce_Activities_QC1 %in% names(tab_heatmap)[tab_heatmap>60],]
    #title=paste(levels(sid)[i],levels(D_id)[j])
    
    print("heelo Covid")
    title=Title
    DF_Sub_Day$Onto_sep<-DF_Sub_Day$Reduce_Activities_QC1
    DF_Sub_Day<-DF_Sub_Day %>% separate(Onto_sep, c("A", "B"))
    print("After seperatet")
    for (k in 1:nrow(DF_Sub_Day)){
      if(is.na(DF_Sub_Day$B[k])){
        DF_Sub_Day$B[k]<-DF_Sub_Day$A[k]
      }
    }
      mytable<-as.data.frame(table(DF_Sub_Day$A, DF_Sub_Day$B))
      colnames(mytable)<-c("A", "B", "Time")
      mytable<-subset(mytable, Time>0)
      
      mytable$A<-as.character(mytable$A)
      mytable$B<-as.character(mytable$B)
      mytable<-mytable[!(mytable$A=="NA" | mytable$A=="Working"),]
      
      mytable_Sym <- data.frame(A = c(mytable[,"A"], mytable[,"B"]),
                                B = c(mytable[,"B"], mytable[,"A"]),
                                Time = c(mytable[,"Time"], mytable[,"Time"]))
      mytable_Sym$A<-as.factor(mytable_Sym$A)
      mytable_Sym$B<-as.factor(mytable_Sym$B)
      print("I am before rest of Ontologies")
      
      if (length(levels(factor(mytable_Sym$A)))<6){
              rest_of_ontologies<-data.frame(A=setdiff(levels(factor(X)),levels(factor(mytable_Sym$A))),
                       B=setdiff(levels(factor(X)),levels(factor(mytable_Sym$A))),
                       Time=0)
                      mytable_Sym<-rbind(mytable_Sym, rest_of_ontologies)
                      print("inside if")
        
      }
        mytable_Sym<-mytable_Sym[!(mytable_Sym$A=="OB"),]
        mytable_mat<-acast(mytable_Sym, A~B, value.var='Time', fun.aggregate=mean)
        mytable_mat<-mytable_mat[,c("R", "W", "Out","SA", "SP","I")]
        mytable_mat<-mytable_mat[c("R", "W", "Out","SA", "SP","I"),]
        # mytable_mat <- mytable_mat[, order(colnames(mytable_mat))]
        # mytable_mat <- mytable_mat[order(rownames(mytable_mat)),]

        
        
        mytable_mat[lower.tri(mytable_mat)] <- NA
        mytable_mat[is.nan(mytable_mat)] <- 0
        mat <- melt(mytable_mat,na.rm = TRUE)

  mat$percent<-percent(mat$value/sum(mat$value))
  mat$percent<-percent(mat$value/sum(mat$value))
  mat$percent_in_num<-(mat$value/sum(mat$value))*100
  mat$percent_in_num<-round(mat$percent_in_num, 2)

  
  mat$Var1 = as.factor(mat$Var1)
  mat$Var2 = as.factor(mat$Var2)
  mat = mat %>% mutate(value2 = ifelse(Var1==Var2, value, NA))
  mat = mat %>% mutate(value1 = ifelse(Var1==Var2, -1, value))
  mat$value1[is.na(mat$value1)] <- -2
  
  
  print("Before")
  #mat<-mat[mat[,"value1"]== -1,]
  mat <- filter(mat, value1 == -1)
  mat<-subset(mat, select = c(Var1, percent_in_num))
  print("after subset")
  
  transpose_Data<-tidyr::spread(mat, Var1, percent_in_num)
  transpose_Data$Participant_ID<-subject_title
  transpose_Data$Day<-Title
  
  
  transpose_Data<-transpose_Data[,c(7,8,1,2,3,4,5,6)]
  
 Activity_data<- rbind(Activity_data, transpose_Data)
        
        
      
  }

}
file_Name<-"Activity_Data.csv"
#write.csv(Activity_data,file.path(data_dir,file_Name ), )
write.table(Activity_data, file.path(data_dir,file_Name ), row.names=F, sep = ',')
```





```{r}

####for testing
DF_Sub_Day<-subset(DF, Participant_ID=="T001" & Day=="Day4" & Treatment=="WS")
#Get_Activity_Data(DF_Sub_Day, "T001", "Day4")
#Get_Activity_Data<-function(DF_Sub_Day,subject_title,Title){
  
    X<-c("R", "W", "Out","SA", "SP","I")
    print(X)
    tab_heatmap <- table(DF_Sub_Day$Reduce_Activities_QC1)
    DF_Sub_Day<-DF_Sub_Day[DF_Sub_Day$Reduce_Activities_QC1 %in% names(tab_heatmap)[tab_heatmap>60],]
    #title=paste(levels(sid)[i],levels(D_id)[j])
    
    print("heelo Covid")
    title=Title
    DF_Sub_Day$Onto_sep<-DF_Sub_Day$Reduce_Activities_QC1
    DF_Sub_Day<-DF_Sub_Day %>% separate(Onto_sep, c("A", "B"))
    print("After seperatet")
    for (k in 1:nrow(DF_Sub_Day)){
      if(is.na(DF_Sub_Day$B[k])){
        DF_Sub_Day$B[k]<-DF_Sub_Day$A[k]
      }
    }
      mytable<-as.data.frame(table(DF_Sub_Day$A, DF_Sub_Day$B))
      colnames(mytable)<-c("A", "B", "Time")
      mytable<-subset(mytable, Time>0)
      
      mytable$A<-as.character(mytable$A)
      mytable$B<-as.character(mytable$B)
      mytable<-mytable[!(mytable$A=="NA" | mytable$A=="Working"),]
      
      mytable_Sym <- data.frame(A = c(mytable[,"A"], mytable[,"B"]),
                                B = c(mytable[,"B"], mytable[,"A"]),
                                Time = c(mytable[,"Time"], mytable[,"Time"]))
      mytable_Sym$A<-as.factor(mytable_Sym$A)
      mytable_Sym$B<-as.factor(mytable_Sym$B)
      print("I am before rest of Ontologies")
      
      if (length(levels(factor(mytable_Sym$A)))<6){
              rest_of_ontologies<-data.frame(A=setdiff(levels(factor(X)),levels(factor(mytable_Sym$A))),
                       B=setdiff(levels(factor(X)),levels(factor(mytable_Sym$A))),
                       Time=0)
                      mytable_Sym<-rbind(mytable_Sym, rest_of_ontologies)
                      print("inside if")
        
      }
        mytable_Sym<-mytable_Sym[!(mytable_Sym$A=="OB"),]
        mytable_mat<-acast(mytable_Sym, A~B, value.var='Time', fun.aggregate=mean)
        mytable_mat<-mytable_mat[,c("R", "W", "Out","SA", "SP","I")]
        mytable_mat<-mytable_mat[c("R", "W", "Out","SA", "SP","I"),]
        # mytable_mat <- mytable_mat[, order(colnames(mytable_mat))]
        # mytable_mat <- mytable_mat[order(rownames(mytable_mat)),]

        
        
        mytable_mat[lower.tri(mytable_mat)] <- NA
        mytable_mat[is.nan(mytable_mat)] <- 0
        mat <- melt(mytable_mat,na.rm = TRUE)

  mat$percent<-percent(mat$value/sum(mat$value))
  mat$percent<-percent(mat$value/sum(mat$value))
  mat$percent_in_num<-(mat$value/sum(mat$value))*100
  mat$percent_in_num<-round(mat$percent_in_num, 2)

  
  mat$Var1 = as.factor(mat$Var1)
  mat$Var2 = as.factor(mat$Var2)
  mat = mat %>% mutate(value2 = ifelse(Var1==Var2, value, NA))
  mat = mat %>% mutate(value1 = ifelse(Var1==Var2, -1, value))
  mat$value1[is.na(mat$value1)] <- -2
  
  
  print("Before")
  #mat<-mat[mat[,"value1"]== -1,]
  mat <- filter(mat, value1 == -1)
  mat<-subset(mat, select = c(Var1, value2))
  print("after subset")
  
  transpose_Data<-tidyr::spread(mat, Var1, value2)
  transpose_Data$Participant_ID<-subject_title
  transpose_Data$Day<-Title
  
  
  transpose_Data<-transpose_Data[,c(7,8,1,3,2,4,5,6)]
  
 Activity_data<- rbind(Activity_data, transpose_Data)
#}
```




